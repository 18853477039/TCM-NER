{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-20T09:08:38.211838Z",
     "start_time": "2025-03-20T09:08:37.261836Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": "from datasets import load_dataset\n",
   "id": "initial_id"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T12:16:51.780932Z",
     "start_time": "2025-03-20T12:16:51.698317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_bio_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "    line_num = 0\n",
    "\n",
    "    for line in lines:\n",
    "        try:\n",
    "            line_num += 1\n",
    "            line = line.rstrip()\n",
    "            if line:\n",
    "                # 处理行首有空格的情况\n",
    "                if line.startswith(' '):  # 如果行首有空格\n",
    "                    token = ' '  # 将空格作为 token\n",
    "                    label = line[1:].strip()  # 剩余部分作为 label\n",
    "                else:\n",
    "                    # 正常情况：按第一个空格分割\n",
    "                    parts = line.split(maxsplit=1)\n",
    "                    if len(parts) == 1:\n",
    "                        # 如果只有一位（例如 \"O\"），则默认标签为 \"O\"\n",
    "                        token = parts[0]\n",
    "                        label = \"O\"\n",
    "                    else:\n",
    "                        # 正常情况：token 和 label 分开\n",
    "                        token, label = parts\n",
    "                current_tokens.append(token)\n",
    "                current_labels.append(label)\n",
    "            else:\n",
    "                # 如果遇到空行，表示一个句子的结束\n",
    "                if current_tokens:\n",
    "                    tokens.append(current_tokens)\n",
    "                    labels.append(current_labels)\n",
    "                    current_tokens = []\n",
    "                    current_labels = []\n",
    "        except:\n",
    "            print(f\"Error in line {line_num}: {line}\")\n",
    "            pass\n",
    "\n",
    "    # 添加最后一个句子（如果存在）\n",
    "    if current_tokens:\n",
    "        tokens.append(current_tokens)\n",
    "        labels.append(current_labels)\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "# 假设你的BIO数据文件名为 'bio_data.txt'\n",
    "file_path = '../datasets/medical.train'\n",
    "tokens, labels = load_bio_data(file_path)\n",
    "\n",
    "# 打印前5个句子及其标签\n",
    "for i in range(5):\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    print(\"Tokens:\", tokens[i])\n",
    "    print(\"Labels:\", labels[i])\n",
    "    print()"
   ],
   "id": "db7789d39c72f9f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "Tokens: ['不', '是']\n",
      "Labels: ['O', 'O']\n",
      "\n",
      "Sentence 2:\n",
      "Tokens: ['现', '头', '昏', '口', '苦']\n",
      "Labels: ['O', 'O', 'O', 'B-临床表现', 'I-临床表现']\n",
      "\n",
      "Sentence 3:\n",
      "Tokens: ['目', '的', '观', '察', '复', '方', '丁', '香', '开', '胃', '贴', '外', '敷', '神', '阙', '穴', '治', '疗', '慢', '性', '心', '功', '能', '不', '全', '伴', '功', '能', '性', '消', '化', '不', '良', '的', '临', '床', '疗', '效']\n",
      "Labels: ['O', 'O', 'O', 'O', 'B-中医治疗', 'I-中医治疗', 'I-中医治疗', 'I-中医治疗', 'I-中医治疗', 'I-中医治疗', 'I-中医治疗', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-西医诊断', 'I-西医诊断', 'I-西医诊断', 'I-西医诊断', 'I-西医诊断', 'I-西医诊断', 'I-西医诊断', 'I-西医诊断', 'I-西医诊断', 'I-西医诊断', 'I-西医诊断', 'I-西医诊断', 'I-西医诊断', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Sentence 4:\n",
      "Tokens: ['舒', '肝', '和', '胃', '消', '痞', '汤', '；', '功', '能', '性', '消', '化', '不', '良']\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-西医诊断', 'I-西医诊断', 'I-西医诊断', 'I-西医诊断', 'I-西医诊断', 'I-西医诊断', 'I-西医诊断']\n",
      "\n",
      "Sentence 5:\n",
      "Tokens: ['患', '者', '３', 'ａ', '前', '咯', '血', '，', '被', '诊', '断', '为', '肺', '结', '核', '，', '住', '院', '４', '０', '余', '天', '时', '出', '现', '腹', '痛', '，', '经', '治', '疗', '好', '转', '，', '但', '时', '有', '发', '作', '，', '坚', '持', '服', '抗', '痨', '药', '３', 'ａ', '后', '，', '因', '腹', '痛', '基', '本', '缓', '解', '，', '肺', '结', '核', '治', '愈', '而', '停', '药']\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'B-临床表现', 'I-临床表现', 'O', 'O', 'O', 'O', 'O', 'B-西医诊断', 'I-西医诊断', 'I-西医诊断', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-西医诊断', 'I-西医诊断', 'I-西医诊断', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T12:26:29.189417Z",
     "start_time": "2025-03-20T12:26:29.177596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def extract_entity_types(labels):\n",
    "    \"\"\"从labels中提取实体类型（去掉B-/I-前缀并去重）\"\"\"\n",
    "    entity_types = set()\n",
    "    for label_list in labels:\n",
    "        for label in label_list:\n",
    "            if label != \"O\":\n",
    "                entity_type = label.split(\"-\")[1]  # 去掉B-/I-前缀\n",
    "                entity_types.add(entity_type)\n",
    "    return list(entity_types)\n",
    "\n",
    "def convert_to_sft_format(tokens, labels, dataset_name=\"MEDICAL\"):\n",
    "    sft_data = []\n",
    "    entity_types = extract_entity_types(labels)  # 提取实体类型\n",
    "\n",
    "    for i, (token_list, label_list) in enumerate(zip(tokens, labels)):\n",
    "        # 构造 input\n",
    "        input_text = \"中医药命名实体识别: \\n\" + \"\".join(token_list) + \"\\n答：\"\n",
    "\n",
    "        # 构造 target\n",
    "        entity_dict = {et: [] for et in entity_types}  # 按实体类型分组\n",
    "        current_entity = None\n",
    "        current_tokens = []\n",
    "\n",
    "        for token, label in zip(token_list, label_list):\n",
    "            if label.startswith(\"B-\"):\n",
    "                # 如果是新的实体，保存上一个实体\n",
    "                if current_entity:\n",
    "                    entity_dict[current_entity].append(\"\".join(current_tokens))\n",
    "                current_entity = label.split(\"-\")[1]  # 提取实体类型\n",
    "                current_tokens = [token]  # 开始新的实体\n",
    "            elif label.startswith(\"I-\"):\n",
    "                # 如果是实体的中间部分，继续添加token\n",
    "                current_tokens.append(token)\n",
    "            else:\n",
    "                # 如果是O，保存上一个实体\n",
    "                if current_entity:\n",
    "                    entity_dict[current_entity].append(\"\".join(current_tokens))\n",
    "                current_entity = None\n",
    "                current_tokens = []\n",
    "\n",
    "        # 保存最后一个实体\n",
    "        if current_entity:\n",
    "            entity_dict[current_entity].append(\"\".join(current_tokens))\n",
    "\n",
    "        # 构造target文本\n",
    "        target_text = \"上述句子中的实体包含：\\n\"\n",
    "        no_entity = True\n",
    "        for et in entity_types:\n",
    "            if entity_dict[et]:\n",
    "                no_entity = False\n",
    "                target_text += f\"{et}实体：{'，'.join(entity_dict[et])}\\n\"\n",
    "        if no_entity:\n",
    "            target_text = \"上述句子没有指定类型实体\"\n",
    "\n",
    "        # 构造样本\n",
    "        sample = {\n",
    "            \"input\": input_text,\n",
    "            \"target\": target_text.strip(),  # 去掉末尾的换行符\n",
    "            # \"answer_choices\": entity_types,\n",
    "            \"task_type\": \"ner\",\n",
    "            \"task_dataset\": dataset_name,\n",
    "            \"sample_id\": f\"train-{i}\"  # 生成唯一的样本ID\n",
    "        }\n",
    "        sft_data.append(sample)\n",
    "\n",
    "    return sft_data\n",
    "\n"
   ],
   "id": "ad182d2ce2fd0a26",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T12:26:30.372846Z",
     "start_time": "2025-03-20T12:26:30.310245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 转换为SFT格式\n",
    "sft_data = convert_to_sft_format(tokens, labels)\n",
    "\n",
    "# 打印第一个样本\n",
    "print(json.dumps(sft_data[0], indent=2, ensure_ascii=False))\n"
   ],
   "id": "4c6184ae74afa673",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"input\": \"中医药命名实体识别: \\n不是\\n答：\",\n",
      "  \"target\": \"上述句子没有指定类型实体\",\n",
      "  \"task_type\": \"ner\",\n",
      "  \"task_dataset\": \"MEDICAL\",\n",
      "  \"sample_id\": \"train-0\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T12:26:33.710716Z",
     "start_time": "2025-03-20T12:26:33.694246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 保存为 .jsonl 文件\n",
    "def save_to_jsonl(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for sample in data:\n",
    "            f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# 保存为 sft_data.jsonl 文件\n",
    "output_file_path = 'train.jsonl'\n",
    "save_to_jsonl(sft_data, output_file_path)\n",
    "\n",
    "print(f\"数据已保存到 {output_file_path}\")"
   ],
   "id": "4bbc07d9f8614242",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已保存到 train.jsonl\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2cab652927ded07a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
